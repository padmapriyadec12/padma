{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting algorithms play a crucial role in dealing with bias variance trade-off.  Unlike bagging algorithms, which only controls for high variance in a model, boosting controls both the aspects (bias & variance), and is considered to be more effective. \n",
    "\n",
    "Boosting is a sequential technique which works on the principle of ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. This technique is followed for a classification problem while a similar technique is used for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting – Essential Tuning Parameters\n",
    "Model complexity and over-fitting can be controlled by using correct values for three categories of parameters.\n",
    "\n",
    "### 1. Tree structure (These effect each individual tree)\n",
    "\n",
    "#### max_depth: \n",
    "    Maximum depth of the individual estimators. The best value depends on the interaction of the input variables.\n",
    "    \n",
    "    max_depth = 8 : Should be chosen (5-8) based on the number of observations and predictors\n",
    "    \n",
    "#### max_leaf_nodes:\n",
    "    The maximum number of terminal nodes or leaves in a tree. Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves. If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "#### min_samples_leaf: \n",
    "    Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "    This will be helpful to ensure sufficient number of samples result in leaf. Used to control over-fitting.\n",
    "    \n",
    "    min_samples_leaf = 50 : Can be selected based on intuition. This is just used for preventing overfitting and again a small value because of imbalanced classes.\n",
    "    \n",
    "#### min_weight_fraction_leaf:\n",
    "    Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer\n",
    "\n",
    "#### min_samples_split:\n",
    "    Defines the minimum number of samples (or observations) which are required in a node to be considered for            splitting. Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "    min_sample_split = 500 : This should be ~0.5-1% of total values. For imbalanced class problem, take a small value from the range.\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "### 2. Regularization parameter (This effect the boosting operations in the model)\n",
    "\n",
    "#### learning_rate: \n",
    "    This controls the magnitude of change in estimators. Lower learning rate is better, which requires higher n_estimators (that is the trade-off). Choose a relatively high learning rate. Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems\n",
    "\n",
    "#### n_estimators: \n",
    "    This is the number of weak learners to be built. Determine the optimum number of trees for this learning rate. This should range around 40-70. Remember to choose a value on which your system can work fairly fast\n",
    "\n",
    "    Lower the learning rate and increase the estimators proportionally to get more robust models.\n",
    "\n",
    "#### subsample: \n",
    "    The fraction of sample to be used for fitting individual models (default=1). Typically .8 (80%) is used to              introduce random selection of samples, which, in turn, increases the robustness against over-fitting.\n",
    "\n",
    "### 3. Miscellaneous Parameters: Other parameters for overall functioning.\n",
    "\n",
    "#### loss:\n",
    "    It refers to the loss function to be minimized in each split. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.\n",
    "    \n",
    "#### init:\n",
    "    This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "    \n",
    "#### random_state\n",
    "    The random number seed so that same random numbers are generated every time. This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\n",
    "    \n",
    "#### verbose:\n",
    "    The type of output to be printed when the model fits. The different values can be:\n",
    "    0: no output generated (default)\n",
    "    1: output generated for trees in certain intervals\n",
    "    >1: output generated for all trees\n",
    "\n",
    "#### warm_start:\n",
    "    Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Analytics\\\\Personal\\\\Machine Learning\\\\Training\\\\R\\\\Dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data in\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a quick logistic regression model and check the accuracy\n",
    "\n",
    "#X = df.iloc[:,:8] # independent variables\n",
    "y = 'Class' # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Class'])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain['Class'], \n",
    "                                                    cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Class'].values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Class'], dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.9062\n",
      "AUC Score (Train): 0.971530\n",
      "CV Score : Mean - 0.8259696 | Std - 0.02850008 | Min - 0.7925 | Max - 0.8775472\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target\n",
    "predictors = df.columns.values[:8]\n",
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "modelfit(gbm0, df, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 1\n",
      "AUC Score (Train): 1.000000\n",
      "CV Score : Mean - 0.8103424 | Std - 0.03942149 | Min - 0.7509259 | Max - 0.8735849\n"
     ]
    }
   ],
   "source": [
    "predictors = df.columns.values[:8]\n",
    "gbm01 = GradientBoostingClassifier(learning_rate=0.1,\n",
    "                                   n_estimators=60,\n",
    "                                   max_depth=9,\n",
    "                                   subsample=0.8,\n",
    "                                   random_state=10)\n",
    "modelfit(gbm01, df, predictors)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bagged Decision Trees for Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('C:\\\\Analytics\\\\Personal\\\\Machine Learning\\\\Training\\\\R\\\\Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data in\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a quick logistic regression model and check the accuracy\n",
    "\n",
    "X = df.iloc[:,:8] # independent variables\n",
    "y = df['Class'] # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "X = preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cross_validation as cross_validation\n",
    "kfold = cross_validation.StratifiedKFold(y = y_train, n_folds=5, random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (stand alone) - Train :  0.698752327026\n",
      "Decision Tree (stand alone) - Test :  0.720779220779\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100\n",
    "# Dection Tree with 5 fold cross validation\n",
    "# lets restrict max_depth to 3 to have more impure leaves\n",
    "clf_DT = DecisionTreeClassifier(max_depth=1, random_state=2017).fit(X_train,y_train)\n",
    "results = cross_validation.cross_val_score(clf_DT, X_train,y_train,cv=kfold)\n",
    "print (\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(clf_DT.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (AdaBoosting) - Train :  0.755730181046\n",
      "Decision Tree (AdaBoosting) - Test :  0.798701298701\n"
     ]
    }
   ],
   "source": [
    "# Using Adaptive Boosting of 100 iteration\n",
    "clf_DT_Boost = AdaBoostClassifier(base_estimator=clf_DT, n_estimators=num_trees, \n",
    "                                  learning_rate=0.1, random_state=2017).fit(X_train,y_train)\n",
    "results = cross_validation.cross_val_score(clf_DT_Boost, X_train, y_train,\n",
    "cv=kfold)\n",
    "print (\"Decision Tree (AdaBoosting) - Train : \", results.mean())\n",
    "print (\"Decision Tree (AdaBoosting) - Test : \", metrics.accuracy_score(clf_DT_Boost.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost (eXtreme Gradient Boosting):\n",
    "\n",
    "It is an extended, more regularized version of a gradient boosting algorithm. Build on C++ as part of the Distributed (Deep) Machine Learning Community. This is one of the most well-performing large-scale,\n",
    "scalable machine learning algorithms that has been playing a major role in winning solutions of Kaggle\n",
    "\n",
    "Some of the key advantages of the xgboost algorithm are these:\n",
    "1. It implements parallel processing.\n",
    "2. It has a built-in standard to handle missing values, which means user can specify a particular value different than other observations (such as -1 or -999) and pass it as a parameter.\n",
    "3. It will split the tree up to a maximum depth unlike Gradient Boosting where it stops splitting node on encounter of a negative loss in the split.\n",
    "\n",
    "XGboost has bundle of parameters, and at a high level we can group them into three categories. Let's look at the most important within these\n",
    "categories.\n",
    "1. General Parameters: \n",
    "   #nthread - Number of parallel threads; if not given a value all cores will be used.\n",
    "   #Booster - This is the type of model to be run with gbtree (tree-based model) being the default. 'gblinear' to be       used for linear models\n",
    "2. Boosting Parameters\n",
    "   #eta - This is the learning rate or step size shrinkage to prevent over-fitting; default is 0.3 and it can range        between 0 to 1\n",
    "   #max_depth - Maximum depth of tree with default being 6.\n",
    "   #min_child_weight - Minimum sum of weights of all observations required in child. Start with 1/square root of event     rate\n",
    "   #colsample_bytree - Fraction of columns to be randomly sampled for each tree with default value of 1.\n",
    "   #Subsample -Fraction of observations to be randomly sampled for each tree with default of value of 1. Lowering this    value makes algorithm conservative to avoid over-fitting.\n",
    "   #lambda - L2 regularization term on weights with default value of 1.\n",
    "   #alpha - L1 regularization term on weight.\n",
    "   \n",
    "3. Task Parameters\n",
    "   #objective - This defines the loss function to be minimized with default value 'reg:linear'. For binary           classification it should be 'binary:logistic' and for multiclass 'multi:softprob' to get the probability value and 'multi:softmax' to get predicted class. For multiclass num_class (number of unique classes) to be specified.\n",
    "   #eval_metric - Metric to be use for validating model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
